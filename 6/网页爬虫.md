```python
# 导入所需的库
import requests

# 你的目标网址，可以根据需要修改
target_url = "https://4399.com"

# 使用requests库获取网页内容
response = requests.get(target_url)

# 检查请求是否成功
if response.status_code == 200:
    # 提取网页中的所有链接
    links = response.text.split('href="')[1:]

    # 创建一个空列表来存储收集到的网址
    collected_urls = []

    # 循环处理每个链接
    for link in links:
        # 提取链接的部分内容，直到遇到双引号
        url = link.split('"')[0]

        # 添加网址到列表中
        collected_urls.append(url)

    # 打印收集到的网址数量
    print(f"Collected {len(collected_urls)} URLs.")

    # 将网址写入文件
    with open("urls.txt", "w") as file:
        for url in collected_urls:
            file.write(url + "\n")

    print("URLs have been saved to urls.txt.")
else:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")

```







```python
# 导入所需的库
import requests

# 你的目标网址，可以根据需要修改
target_url = "https://4399.com"

# 使用requests库获取网页内容
response = requests.get(target_url)

# 创建一个空列表来存储收集到的网址
collected_urls = []

# 检查请求是否成功
if response.status_code == 200:
    with open(f"html {len(collected_urls)}.html", "w", encoding='utf-8') as file:
        file.write(response.text)

    # 提取网页中的所有链接
    links = response.text.split('href="')[1:]

    # 循环处理每个链接
    for link in links:
        # 提取链接的部分内容，直到遇到双引号
        url = link.split('"')[0]

        # 添加网址到列表中
        collected_urls.append(url)

for url in collected_urls:
    response = requests.get(url)
    if response.status_code == 200:
        with open(f"html {len(collected_urls)}.html", "w", encoding='utf-8') as file:
            file.write(response.text)




```







```
import os
import requests
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
}
def save_html(url, file_name, headers=None):
    try:
        response = requests.get(url, headers=headers)  # 将headers传递给requests.get方法
        response.raise_for_status()

        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(response.text)

        print(f"Successfully saved {url} as {file_name}")
    except Exception as e:
        print(f"Failed to save {url}: {str(e)}")



def scrape_web_pages(start_url, max_depth=3):
    visited_urls = set()

    def recursive_scrape(url, depth):
        if depth > max_depth:
            return

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)

            for link in links:
                next_url = link['href']
                if next_url.startswith('http'):
                    if next_url not in visited_urls:
                        visited_urls.add(next_url)
                        file_name = f"html{493 + len(visited_urls)}.html"
                        save_html(next_url, file_name, headers)
                        recursive_scrape(next_url, depth + 1)

        except Exception as e:
            print(f"Failed to scrape {url}: {str(e)}")

    save_html(start_url, "html493.html", headers)
    recursive_scrape(start_url, 1)


if __name__ == "__main__":
    start_url = "https://movie.douban.com"  # 你可以替换为你需要的起始网页
    scrape_web_pages(start_url)

```





```
import os
import requests
from bs4 import BeautifulSoup
import threading

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
}

output_directory = r'E:\pycharm\HTML'

def save_html(url, file_name, headers=None):
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        full_file_path = os.path.join(output_directory, file_name)
        with open(full_file_path, 'w', encoding='utf-8') as f:
            f.write(response.text)

        print(f"Successfully saved {url} as {file_name}")
    except Exception as e:
        print(f"Failed to save {url}: {str(e)}")


def scrape_web_pages(start_url, max_depth=2, num_threads=30):
    visited_urls = set()
    lock = threading.Lock()

    def recursive_scrape(url, depth):
        if depth > max_depth:
            return

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)

            for link in links:
                next_url = link['href']
                if next_url.startswith('http'):
                    with lock:
                        if next_url not in visited_urls:
                            visited_urls.add(next_url)
                            file_name = f"html{12165 + len(visited_urls)}.html"
                            save_html(next_url, file_name, headers)
                            # 使用多线程处理递归调用
                            if depth + 1 <= max_depth:
                                threading.Thread(target=recursive_scrape, args=(next_url, depth + 1)).start()

        except Exception as e:
            print(f"Failed to scrape {url}: {str(e)}")

    save_html(start_url, "html12165.html", headers)

    # 使用多线程处理起始网页的递归调用
    for _ in range(num_threads):
        threading.Thread(target=recursive_scrape, args=(start_url, 1)).start()


if __name__ == "__main__":
    start_url = "https://www.taobao.com"
    scrape_web_pages(start_url)

```





```
import os
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import threading
import atexit

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
}

output_directory = r'E:\pycharm\HTML'
lock = threading.Lock()

def save_html(url, file_name):
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        full_file_path = os.path.join(output_directory, file_name)
        with open(full_file_path, 'wb') as f:
            f.write(response.content)

        print(f"Successfully saved {url} as {file_name}")
    except Exception as e:
        print(f"Failed to save {url}: {str(e)}")

def scrape_web_pages(start_url, max_depth=2, num_threads=30):
    visited_urls = set()

    def recursive_scrape(url, depth):
        if depth > max_depth:
            return

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a', href=True)

            for link in links:
                next_url = link['href']
                if next_url.startswith('http'):
                    with lock:
                        if next_url not in visited_urls:
                            visited_urls.add(next_url)
                            file_name = f"html{12793 + len(visited_urls)}.html"
                            save_html(next_url, file_name)
                            # 使用线程池处理递归调用
                            if depth + 1 <= max_depth:
                                executor.submit(recursive_scrape, next_url, depth + 1)

        except Exception as e:
            print(f"Failed to scrape {url}: {str(e)}")

    save_html(start_url, "html12793.html")

    # 使用线程池处理起始网页的递归调用
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        executor.submit(recursive_scrape, start_url, 1)

# 在程序退出时关闭线程池
def exit_handler():
    executor.shutdown()

if __name__ == "__main__":
    start_url = "https://www.taobao.com"
    executor = ThreadPoolExecutor(max_workers=30)
    atexit.register(exit_handler)
    scrape_web_pages(start_url)

```

